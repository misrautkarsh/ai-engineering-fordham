{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸŽ“ **Professor**: Apostolos Filippas\n",
    "\n",
    "### ðŸ“˜ **Class**: AI Engineering\n",
    "\n",
    "### ðŸ“‹ **Homework 2**: Working with LLMs via API\n",
    "\n",
    "### ðŸ“… **Due Date**: Day of Lecture 3, 11:59 PM\n",
    "\n",
    "#### ðŸ”— **My Repository**: https://github.com/YOUR-USERNAME/ai-engineering-fordham\n",
    "\n",
    "*(Replace the URL above with your actual repository URL)*\n",
    "\n",
    "**Note**: You are not allowed to share the contents of this notebook with anyone outside this class without written permission by the professor.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Movie Poster Generator\n",
    "\n",
    "In this homework, you'll build a mini-application that:\n",
    "1. **Extracts** structured movie data from text descriptions using Pydantic\n",
    "2. **Processes** multiple movies concurrently using async programming\n",
    "3. **Explores** temperature, logprobs, and reasoning models\n",
    "4. **Generates** movie posters using AI image generation\n",
    "\n",
    "This project combines key skills from Lecture 2: structured outputs, async programming, LLM parameters, and image generation.\n",
    "\n",
    "**Total Points: 145** (+ 10 bonus)\n",
    "\n",
    "---\n",
    "\n",
    "### A Note on Using Resources\n",
    "\n",
    "You are encouraged to use any resources to complete this homework:\n",
    "- **ChatGPT / Claude** - Ask AI to explain concepts or help debug\n",
    "- **Lecture 2 notebook** - Reference the examples we covered\n",
    "- **Official documentation** - LiteLLM, Pydantic, Google GenAI docs\n",
    "\n",
    "When you use external resources, please cite them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Environment Setup (10 points)\n",
    "\n",
    "First, let's verify your environment is set up correctly.\n",
    "\n",
    "### 1a. Verify imports work (5 pts)\n",
    "\n",
    "Run the cell below. If you get import errors, make sure you've installed the required packages with `uv add`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1a: Verify imports work (5 pts)\n",
    "import litellm\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import asyncio\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Verify API keys are set (5 pts)\n",
    "\n",
    "Test that your API keys work by making a simple call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import litellm\n",
    "\n",
    "# Load OPENAI_API_KEY from .env into environment variables\n",
    "load_dotenv()\n",
    "\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Say 'API working!' and nothing else.\"}],\n",
    "    max_tokens=20\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Design the Movie Schema (15 points)\n",
    "\n",
    "Design a Pydantic model to represent movie data. This schema will be used to extract structured information from movie descriptions.\n",
    "\n",
    "**Requirements:**\n",
    "- `title` - string, required\n",
    "- `genre` - use `Literal` with at least 4 genre options (e.g., \"sci-fi\", \"drama\", \"action\", \"comedy\", etc.)\n",
    "- `year` - integer with validation (must be between 1900 and 2030)\n",
    "- `main_characters` - list of strings (1-5 characters)\n",
    "- `mood` - string describing the emotional tone\n",
    "- `visual_style` - string describing how the movie looks visually\n",
    "- `tagline` - optional string (the movie's catchphrase)\n",
    "\n",
    "**Hints:**\n",
    "- Use `Field(ge=..., le=...)` for numeric validation\n",
    "- Use `Field(min_length=..., max_length=...)` for list length validation\n",
    "- Use `| None = None` for optional fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Task 2: Design your Movie schema (15 pts)\n",
    "\n",
    "Genre = Literal[\"sci-fi\", \"drama\", \"action\", \"comedy\", \"thriller\", \"horror\", \"romance\", \"fantasy\"]\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    \"\"\"Structured representation of a movie.\"\"\"\n",
    "    title: str\n",
    "    genre: Genre\n",
    "    year: int = Field(ge=1900, le=2030)\n",
    "    main_characters: list[str] = Field(min_length=1, max_length=5)\n",
    "    mood: str\n",
    "    visual_style: str\n",
    "    tagline: str | None = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your schema by creating a Movie object\n",
    "# This should work if your schema is correct\n",
    "\n",
    "test_movie = Movie(\n",
    "    title=\"The Matrix\",\n",
    "    genre=\"sci-fi\",\n",
    "    year=1999,\n",
    "    main_characters=[\"Neo\", \"Morpheus\", \"Trinity\"],\n",
    "    mood=\"Dark and philosophical\",\n",
    "    visual_style=\"Futuristic cyberpunk with green digital tones\",\n",
    "    tagline=\"Welcome to the Real World\"\n",
    ")\n",
    "\n",
    "print(test_movie.model_dump_json(indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: Extract Movie Data with Structured Outputs (20 points)\n",
    "\n",
    "Write a function that takes a movie description and uses LiteLLM with structured outputs to extract a `Movie` object.\n",
    "\n",
    "**Hints:**\n",
    "- Use `litellm.completion()` with `response_format=Movie`\n",
    "- The LLM will automatically return data matching your schema\n",
    "- Parse the JSON response into a Movie object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import litellm\n",
    "\n",
    "# Task 3: Write a function to extract movie data (20 pts)\n",
    "\n",
    "def extract_movie(description: str) -> Movie:\n",
    "    \"\"\"\n",
    "    Use LiteLLM with structured outputs to extract movie data.\n",
    "    \n",
    "    Args:\n",
    "        description: A text description of a movie\n",
    "        \n",
    "    Returns:\n",
    "        A Movie object with the extracted data\n",
    "    \"\"\"\n",
    "    response = litellm.completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Extract the movie information from the user's description and return ONLY valid JSON that matches the provided schema.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": description},\n",
    "        ],\n",
    "        response_format=Movie,\n",
    "    )\n",
    "\n",
    "    # LiteLLM returns JSON in the model message content for structured outputs\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    # Convert JSON -> dict -> Movie\n",
    "    data = json.loads(content)\n",
    "    return Movie(**data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function with this description (Avatar)\n",
    "\n",
    "test_description = \"\"\"\n",
    "The year is 2154. Jake Sully, a paralyzed marine, is sent to the moon Pandora \n",
    "where he falls in love with a native Na'vi woman named Neytiri while on a mission \n",
    "to infiltrate their tribe. The film is a visually stunning sci-fi epic with \n",
    "bioluminescent forests and floating mountains. It explores themes of \n",
    "environmentalism and colonialism with an awe-inspiring, hopeful tone.\n",
    "\"\"\"\n",
    "\n",
    "movie = extract_movie(test_description)\n",
    "print(movie.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Async Batch Processing (20 points)\n",
    "\n",
    "Now let's process multiple movies concurrently! This is much faster than processing them one at a time.\n",
    "\n",
    "### 4a. Write an async version of extract_movie (10 pts)\n",
    "\n",
    "**Hints:**\n",
    "- Use `async def` instead of `def`\n",
    "- Use `await litellm.acompletion()` instead of `litellm.completion()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import litellm\n",
    "\n",
    "# Task 4a: Write an async version of extract_movie (10 pts)\n",
    "\n",
    "async def async_extract_movie(description: str) -> Movie:\n",
    "    \"\"\"Extract movie data asynchronously.\"\"\"\n",
    "    response = await litellm.acompletion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Extract the movie information from the user's description and return ONLY valid JSON that matches the provided schema.\"\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": description},\n",
    "        ],\n",
    "        response_format=Movie,\n",
    "    )\n",
    "\n",
    "    content = response.choices[0].message.content\n",
    "    data = json.loads(content)\n",
    "    return Movie(**data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Process all descriptions concurrently (10 pts)\n",
    "\n",
    "**Hints:**\n",
    "- Create a list of tasks using list comprehension\n",
    "- Use `asyncio.gather(*tasks)` to run them all concurrently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are 5 movie descriptions to process:\n",
    "movie_descriptions = [\n",
    "    \"\"\"A dinosaur theme park on a remote island goes terribly wrong when the security \n",
    "    systems fail during a tropical storm. Scientists and visitors must survive against \n",
    "    escaped prehistoric predators. Directed with Spielberg's signature sense of wonder \n",
    "    and terror, featuring groundbreaking CGI dinosaurs.\"\"\",\n",
    "    \n",
    "    \"\"\"A young boy discovers on his 11th birthday that he's actually a famous wizard \n",
    "    in the magical world. He attends a school for witchcraft where he makes friends, \n",
    "    learns magic, and uncovers the mystery of his parents' death. A whimsical fantasy \n",
    "    with gothic British atmosphere.\"\"\",\n",
    "    \n",
    "    \"\"\"In a world where skilled thieves can enter people's dreams to steal secrets, \n",
    "    one man is offered a chance to have his criminal record erased if he can do the \n",
    "    impossible: plant an idea in someone's mind. A mind-bending thriller with \n",
    "    rotating hallways and cities folding on themselves.\"\"\",\n",
    "    \n",
    "    \"\"\"A young lion prince is tricked by his uncle into thinking he caused his \n",
    "    father's death and flees into exile. Years later, he must return to reclaim \n",
    "    his kingdom. An animated musical epic set on the African savanna with \n",
    "    stunning hand-drawn animation.\"\"\",\n",
    "    \n",
    "    \"\"\"In a dystopian future where Earth is dying, a team of astronauts travels \n",
    "    through a wormhole near Saturn to find a new home for humanity. A father \n",
    "    must choose between seeing his children again and saving the human race. \n",
    "    Epic space visuals with an emotional core.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "# Task 4b: Process all descriptions concurrently (10 pts)\n",
    "\n",
    "async def extract_all_movies(descriptions: list[str]) -> list[Movie]:\n",
    "    \"\"\"Process all movie descriptions concurrently and return results.\"\"\"\n",
    "    tasks = [async_extract_movie(desc) for desc in descriptions]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and time it!\n",
    "start = time.time()\n",
    "movies = await extract_all_movies(movie_descriptions)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Processed {len(movies)} movies in {elapsed:.2f} seconds\")\n",
    "print()\n",
    "for m in movies:\n",
    "    print(f\"  - {m.title} ({m.year}) - {m.genre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 5: Understanding Temperature (15 points)\n",
    "\n",
    "Temperature controls how \"random\" or \"creative\" an LLM's outputs are:\n",
    "\n",
    "| Temperature | Behavior |\n",
    "|-------------|----------|\n",
    "| **0.0** | Deterministic - always picks the most likely token |\n",
    "| **0.7** | Balanced - some creativity while staying coherent |\n",
    "| **1.0** | Default - moderate randomness |\n",
    "| **1.5+** | High creativity - more surprising/diverse outputs |\n",
    "\n",
    "### 5a. Temperature Comparison (10 pts)\n",
    "\n",
    "Run the same creative prompt at different temperatures (0.0, 0.7, 1.0, 1.5) **three times each**. Observe:\n",
    "- At temperature 0, do you get the same output every time?\n",
    "- How does creativity/variety change as temperature increases?\n",
    "\n",
    "**Hints:**\n",
    "- Use `temperature=X` parameter in `litellm.completion()`\n",
    "- Use the provided prompt about movie taglines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "# Task 5a: Temperature Comparison (10 pts)\n",
    "\n",
    "creative_prompt = \"Write a one-sentence movie tagline for a sci-fi thriller about AI.\"\n",
    "temperatures = [0.0, 0.7, 1.0, 1.5]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    for i in range(3):\n",
    "        response = litellm.completion(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": creative_prompt}],\n",
    "            temperature=temp,\n",
    "            max_tokens=30\n",
    "        )\n",
    "        \n",
    "        print(f\"Run {i+1}: {response.choices[0].message.content.strip()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Analyze Output Diversity (5 pts)\n",
    "\n",
    "Write a function that generates N completions at a given temperature and measures how diverse the outputs are.\n",
    "\n",
    "**Hints:**\n",
    "- Generate multiple completions and count unique outputs\n",
    "- A simple diversity metric: `unique_outputs / total_outputs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "# Task 5b: Analyze Output Diversity (5 pts)\n",
    "\n",
    "def measure_diversity(prompt: str, temperature: float, n_samples: int = 5) -> dict:\n",
    "    \"\"\"\n",
    "    Generate n_samples completions and measure diversity.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The prompt to send to the LLM\n",
    "        temperature: Temperature setting (0.0 to 2.0)\n",
    "        n_samples: Number of completions to generate\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'outputs' (list), 'unique_count' (int), 'diversity_ratio' (float)\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        response = litellm.completion(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=30\n",
    "        )\n",
    "        text = response.choices[0].message.content.strip()\n",
    "        outputs.append(text)\n",
    "\n",
    "    unique_outputs = set(outputs)\n",
    "    unique_count = len(unique_outputs)\n",
    "    diversity_ratio = unique_count / n_samples if n_samples > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"outputs\": outputs,\n",
    "        \"unique_count\": unique_count,\n",
    "        \"diversity_ratio\": diversity_ratio\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your diversity function\n",
    "test_prompt = \"Name a color.\"\n",
    "\n",
    "print(\"Testing diversity at different temperatures:\\n\")\n",
    "for temp in [0.0, 1.0, 1.5]:\n",
    "    result = measure_diversity(test_prompt, temperature=temp, n_samples=5)\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(f\"  Outputs: {result['outputs']}\")\n",
    "    print(f\"  Unique: {result['unique_count']}/{5}\")\n",
    "    print(f\"  Diversity ratio: {result['diversity_ratio']:.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 6: Understanding Logprobs (15 points)\n",
    "\n",
    "**Logprobs** (log probabilities) let you see \"inside\" the model's decision-making. For each token generated, you can see:\n",
    "- The probability the model assigned to the chosen token\n",
    "- Alternative tokens the model considered (and their probabilities)\n",
    "\n",
    "This helps you understand:\n",
    "- How \"confident\" the model is in its outputs\n",
    "- What other options it was considering\n",
    "- Why certain generations might be more reliable than others\n",
    "\n",
    "### 6a. Request and View Logprobs (10 pts)\n",
    "\n",
    "Make a completion request with `logprobs=True` and `top_logprobs=5` to see the top 5 token alternatives for each position.\n",
    "\n",
    "**Hints:**\n",
    "- Add `logprobs=True` and `top_logprobs=5` to your completion call\n",
    "- Access logprobs via `response.choices[0].logprobs.content`\n",
    "- Each token has a `top_logprobs` list with alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6a: Request and View Logprobs (10 pts)\n",
    "# Note: We use gpt-4o-mini which supports logprobs parameter\n",
    "\n",
    "import math\n",
    "import litellm\n",
    "\n",
    "# Make a completion request with logprobs enabled\n",
    "response = litellm.completion(\n",
    "    model=\"gpt-4o-mini\",  # Use gpt-4o-mini which supports logprobs\n",
    "    messages=[{\"role\": \"user\", \"content\": \"The capital of France is\"}],\n",
    "    max_tokens=10,\n",
    "    logprobs=True,\n",
    "    top_logprobs=5  # Get top 5 alternatives for each token\n",
    ")\n",
    "\n",
    "# 1. Print the generated text\n",
    "print(\"Generated text:\", response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Token-by-token analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 2. Access response.choices[0].logprobs.content\n",
    "logprob_data = response.choices[0].logprobs.content\n",
    "\n",
    "# 3. For each token, print the token and its top 5 alternatives with probabilities\n",
    "for token_info in logprob_data:\n",
    "    token = token_info.token\n",
    "    chosen_logprob = token_info.logprob\n",
    "    chosen_prob = math.exp(chosen_logprob)\n",
    "\n",
    "    print(f\"\\nToken: '{token}'\")\n",
    "    print(f\"Chosen token probability: {chosen_prob:.4f}\")\n",
    "    print(\"Top alternatives:\")\n",
    "\n",
    "    for alt in token_info.top_logprobs:\n",
    "        alt_token = alt.token\n",
    "        alt_prob = math.exp(alt.logprob)\n",
    "        print(f\"  {alt_token:>12} â†’ {alt_prob:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b. Visualize Token Probabilities (5 pts)\n",
    "\n",
    "Create a simple visualization showing the probability distribution for a specific token position. You can use a bar chart or ASCII art.\n",
    "\n",
    "**Hints:**\n",
    "- Pick an interesting token position (e.g., where the model had to make a choice)\n",
    "- Convert logprobs to probabilities using `math.exp(logprob)`\n",
    "- A simple bar chart: `\"â–ˆ\" * int(prob * 50)` gives you ASCII bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 6b: Visualize Token Probabilities (5 pts)\n",
    "\n",
    "import math\n",
    "\n",
    "def visualize_token_probs(logprobs_content, token_index: int = 0, width: int = 50):\n",
    "    \"\"\"\n",
    "    Visualize the probability distribution for a specific token position.\n",
    "\n",
    "    Args:\n",
    "        logprobs_content: response.choices[0].logprobs.content\n",
    "        token_index: which token position to visualize (0 = first token)\n",
    "        width: bar width for ASCII chart\n",
    "    \"\"\"\n",
    "    if not logprobs_content:\n",
    "        print(\"No logprobs data available.\")\n",
    "        return\n",
    "\n",
    "    if token_index < 0 or token_index >= len(logprobs_content):\n",
    "        print(f\"Token index out of range. Must be 0..{len(logprobs_content)-1}\")\n",
    "        return\n",
    "\n",
    "    token_info = logprobs_content[token_index]\n",
    "\n",
    "    # Build a dict of token -> probability (deduplicate!)\n",
    "    # Start with alternatives\n",
    "    token_to_prob = {}\n",
    "    for alt in token_info.top_logprobs:\n",
    "        token_to_prob[alt.token] = math.exp(alt.logprob)\n",
    "\n",
    "    # Ensure chosen token is included (and not duplicated)\n",
    "    token_to_prob[token_info.token] = math.exp(token_info.logprob)\n",
    "\n",
    "    # Normalize for display (since top_logprobs is truncated, sum may be < 1)\n",
    "    total = sum(token_to_prob.values())\n",
    "    if total <= 0:\n",
    "        print(\"Probabilities invalid (sum <= 0).\")\n",
    "        return\n",
    "\n",
    "    items = [(tok, p / total) for tok, p in token_to_prob.items()]\n",
    "    items.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"\\nVisualizing token position {token_index}\")\n",
    "    print(f\"Chosen token: '{token_info.token}'\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for tok, prob in items:\n",
    "        bar = \"â–ˆ\" * int(prob * width)\n",
    "        print(f\"{tok:>12} | {bar:<{width}} {prob:6.2%}\")\n",
    "\n",
    "# âœ… Call it (example)\n",
    "visualize_token_probs(response.choices[0].logprobs.content, token_index=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 7: Reasoning Models (15 points)\n",
    "\n",
    "**Reasoning models** like OpenAI's o3-mini are designed to \"think through\" complex problems before answering. They:\n",
    "- Break down problems into steps\n",
    "- Consider multiple approaches\n",
    "- Show their reasoning process\n",
    "- Excel at logic puzzles, math, and code\n",
    "\n",
    "### 7a. Using o3-mini for Complex Reasoning (10 pts)\n",
    "\n",
    "Use OpenAI's o3-mini reasoning model through LiteLLM to solve a complex logic puzzle.\n",
    "\n",
    "**Hints:**\n",
    "- Use `model=\"o3-mini\"` in your litellm call\n",
    "- Reasoning models work best with challenging problems\n",
    "- Observe how the response shows step-by-step thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "\n",
    "# Task 7a: Using o3-mini for Complex Reasoning (10 pts)\n",
    "\n",
    "logic_puzzle = \"\"\"\n",
    "Three friends (Alice, Bob, and Carol) each have a different pet (cat, dog, fish) \n",
    "and a different favorite color (red, blue, green).\n",
    "\n",
    "Clues:\n",
    "1. Alice doesn't have the cat.\n",
    "2. The person with the dog likes blue.\n",
    "3. Carol likes green.\n",
    "4. Bob doesn't have the fish.\n",
    "\n",
    "Who has which pet and what is their favorite color?\n",
    "Solve this step by step.\n",
    "\"\"\"\n",
    "\n",
    "response = litellm.completion(\n",
    "    model=\"o3-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Solve the puzzle step by step and clearly state the final assignments.\"},\n",
    "        {\"role\": \"user\", \"content\": logic_puzzle},\n",
    "    ],\n",
    "    max_tokens=400,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7b. Compare Reasoning vs Non-Reasoning (5 pts)\n",
    "\n",
    "Now solve the same puzzle using `gpt-5-mini` (a non-reasoning model) and compare the results.\n",
    "\n",
    "**Questions to consider:**\n",
    "- Does the non-reasoning model show step-by-step thinking?\n",
    "- Which model gets the correct answer?\n",
    "- How does the response structure differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "import json\n",
    "\n",
    "logic_puzzle = \"\"\"\n",
    "Three friends (Alice, Bob, and Carol) each have a different pet (cat, dog, fish) \n",
    "and a different favorite color (red, blue, green).\n",
    "\n",
    "Clues:\n",
    "1. Alice doesn't have the cat.\n",
    "2. The person with the dog likes blue.\n",
    "3. Carol likes green.\n",
    "4. Bob doesn't have the fish.\n",
    "\n",
    "Who has which pet and what is their favorite color?\n",
    "Solve this step by step, then give final assignments.\n",
    "\"\"\"\n",
    "\n",
    "def safe_print_response(label, resp):\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(label)\n",
    "    print(\"=\"*70)\n",
    "    content = (resp.choices[0].message.content or \"\").strip()\n",
    "    if content:\n",
    "        print(content)\n",
    "    else:\n",
    "        # Debug info if content is blank\n",
    "        print(\"[BLANK OUTPUT]\")\n",
    "        try:\n",
    "            print(\"finish_reason:\", getattr(resp.choices[0], \"finish_reason\", None))\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Print a compact view of the raw response (helps diagnose)\n",
    "        try:\n",
    "            print(\"raw (truncated):\")\n",
    "            raw = resp.model_dump() if hasattr(resp, \"model_dump\") else resp.__dict__\n",
    "            print(json.dumps(raw, indent=2)[:1500])\n",
    "        except Exception as e:\n",
    "            print(\"Could not dump raw response:\", e)\n",
    "\n",
    "# --- o3-mini (reasoning model) ---\n",
    "resp_reasoning = litellm.completion(\n",
    "    model=\"o3-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Solve step-by-step, then give final assignments in a clear list.\"},\n",
    "        {\"role\": \"user\", \"content\": logic_puzzle},\n",
    "    ],\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "# --- gpt-5-mini (non-reasoning model) ---\n",
    "resp_standard = litellm.completion(\n",
    "    model=\"gpt-5-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Solve the puzzle and give final assignments in a clear list.\"},\n",
    "        {\"role\": \"user\", \"content\": logic_puzzle},\n",
    "    ],\n",
    "    max_tokens=500,\n",
    ")\n",
    "\n",
    "safe_print_response(\"o3-mini (reasoning) response\", resp_reasoning)\n",
    "safe_print_response(\"gpt-5-mini (non-reasoning) response\", resp_standard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 8: Generate Movie Poster (20 points)\n",
    "\n",
    "Now for the fun part - generating movie posters using AI!\n",
    "\n",
    "### 8a. Design a prompt generator (5 pts)\n",
    "\n",
    "Write a function that takes a `Movie` object and creates a detailed image generation prompt.\n",
    "\n",
    "**Your prompt should incorporate:**\n",
    "- The movie's visual style\n",
    "- The mood/tone\n",
    "- Key visual elements that represent the genre\n",
    "- Professional movie poster composition\n",
    "\n",
    "**Tip:** Aim for 50-100 words. Be specific about colors, composition, and style!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poster_prompt(movie: Movie) -> str:\n",
    "    \"\"\"\n",
    "    Create a vivid, scene-based movie poster prompt (50â€“100 words).\n",
    "    Works even if movie fields are generic.\n",
    "    \"\"\"\n",
    "\n",
    "    genre_scenes = {\n",
    "        \"sci-fi\": \"a vast futuristic landscape with glowing structures, alien skies, and advanced tech interfaces\",\n",
    "        \"action\": \"an explosive moment frozen in time with motion blur, debris, and dramatic camera tilt\",\n",
    "        \"thriller\": \"a tense scene with deep shadows, mist, and a single subject caught in a beam of light\",\n",
    "        \"fantasy\": \"a magical realm with towering landscapes, glowing symbols, and ethereal light rays\",\n",
    "        \"drama\": \"an emotionally charged close-up scene in a realistic environment with cinematic lighting\",\n",
    "        \"horror\": \"a dark, eerie setting with fog, silhouettes, and unsettling negative space\",\n",
    "        \"comedy\": \"a bright, playful scene with exaggerated poses and vibrant color contrasts\",\n",
    "        \"romance\": \"an intimate moment framed by warm sunset tones and soft depth of field\"\n",
    "    }\n",
    "\n",
    "    scene = genre_scenes.get(movie.genre, \"a cinematic environment with dramatic lighting and atmosphere\")\n",
    "\n",
    "    mood = movie.mood.lower() if movie.mood else \"cinematic\"\n",
    "    style = movie.visual_style.lower() if movie.visual_style else \"\"\n",
    "\n",
    "    # Use characters only if they look like names\n",
    "    characters = [\n",
    "        c for c in (movie.main_characters or [])\n",
    "        if c and c[0].isupper() and \" \" not in c.lower()\n",
    "    ]\n",
    "\n",
    "    character_text = (\n",
    "        f\"Foreground features {', '.join(characters[:2])} in sharp focus, \"\n",
    "        if characters else\n",
    "        \"Foreground shows a central figure in sharp focus, \"\n",
    "    )\n",
    "\n",
    "    prompt = (\n",
    "        f\"Epic theatrical movie poster for '{movie.title}'. \"\n",
    "        f\"Scene: {scene}. \"\n",
    "        f\"Tone is {mood}. \"\n",
    "        f\"{character_text}\"\n",
    "        \"midground filled with environmental storytelling elements, \"\n",
    "        \"and a grand atmospheric background creating depth and scale. \"\n",
    "        f\"Visual style includes {style} textures, dramatic rim lighting, volumetric light beams, \"\n",
    "        \"cinematic color grading, ultra-detailed realism, sharp focus, and professional studio poster composition \"\n",
    "        \"with space for title typography and tagline.\"\n",
    "    )\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your prompt generator\n",
    "chosen_movie = movies[0]  # or pick your favorite from the list!\n",
    "prompt = generate_poster_prompt(chosen_movie)\n",
    "\n",
    "print(f\"Prompt for '{chosen_movie.title}':\")\n",
    "print()\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8b. Generate the actual image (10 pts)\n",
    "\n",
    "Use Google's Gemini to generate the movie poster.\n",
    "\n",
    "**Hints:**\n",
    "- Use `genai.Client()` to create a client\n",
    "- Use `client.models.generate_content()` with `model=\"gemini-2.5-flash-image\"`\n",
    "- The response will have an image in `response.candidates[0].content.parts`\n",
    "- Save the image to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# 1. Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# 2. Initialize the OpenAI client\n",
    "# It will automatically look for the \"OPENAI_API_KEY\" variable\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Make sure to create temp directory\n",
    "os.makedirs(\"temp\", exist_ok=True)\n",
    "\n",
    "# 3. Define the prompt (assuming chosen_movie is defined elsewhere in your script)\n",
    "# movie_title = chosen_movie.title \n",
    "movie_title = \"Movie\" # Placeholder for testing\n",
    "prompt = f\"A high-quality, cinematic movie poster for a film titled '{movie_title}'. Sci-fi aesthetic, vibrant nebula colors, professional typography.\"\n",
    "\n",
    "print(f\"Generating poster for '{movie_title}' via DALL-E 3...\")\n",
    "\n",
    "try:\n",
    "    # 4. Generate the image using DALL-E 3\n",
    "    response = client.images.generate(\n",
    "        model=\"dall-e-3\",\n",
    "        prompt=prompt,\n",
    "        size=\"1024x1024\",\n",
    "        quality=\"standard\",\n",
    "        n=1,\n",
    "    )\n",
    "\n",
    "    # OpenAI returns a URL for the generated image\n",
    "    image_url = response.data[0].url\n",
    "    \n",
    "    # 5. Download and save the image\n",
    "    if image_url:\n",
    "        image_data = requests.get(image_url).content\n",
    "        \n",
    "        safe_title = movie_title.replace(\" \", \"_\").replace(\"'\", \"\").lower()\n",
    "        file_path = f\"temp/poster_{safe_title}.png\"\n",
    "        \n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(image_data)\n",
    "        print(f\"Success! Poster saved to: {file_path}\")\n",
    "    else:\n",
    "        print(\"Generation failed: No image URL returned.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8c. Display the image (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 8c: Display the saved image (5 pts)\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Construct the file path used in Task 8b\n",
    "movie_title = \"Movie\"  # Ensure this matches the title from 8b\n",
    "file_path = f\"temp/poster_{movie_title}.png\"\n",
    "\n",
    "# Verify the file exists and display it\n",
    "if os.path.exists(file_path):\n",
    "    display(Image(filename=file_path))\n",
    "else:\n",
    "    print(f\"âŒ Error: Could not find the file at {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 9: Submit via Pull Request (15 points)\n",
    "\n",
    "Now let's practice a real-world development workflow! Instead of pushing directly to `main`, you'll create a **branch**, open a **Pull Request (PR)**, and **merge** it.\n",
    "\n",
    "This is how professional developers submit code for review. Your TA will check your merged PR to verify your submission.\n",
    "\n",
    "### 9a. Create a new branch (5 pts)\n",
    "\n",
    "Run this command in your terminal to create and switch to a new branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9a: Create a new branch (5 pts)\n",
    "# Run this in your terminal (not in this notebook!)\n",
    "\n",
    "!git checkout -b homework-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9b. Commit your work (5 pts)\n",
    "\n",
    "Stage all your changes and create a commit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9b: Commit your work (5 pts)\n",
    "\n",
    "!git add .\n",
    "!git commit -m \"Complete homework 2: Movie Poster Generator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 9c: Push your branch (5 pts)\n",
    "\n",
    "!git push -u origin homework-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9d. Create and Merge the Pull Request\n",
    "\n",
    "Now go to your repository on GitHub (https://github.com/YOUR-USERNAME/ai-engineering-fordham):\n",
    "\n",
    "1. You should see a banner saying **\"homework-2 had recent pushes\"** - click **\"Compare & pull request\"**\n",
    "2. Give your PR a title: `\"Homework 2: Movie Poster Generator\"`\n",
    "3. Click **\"Create pull request\"**\n",
    "4. Review your changes in the PR\n",
    "5. Click **\"Merge pull request\"** then **\"Confirm merge\"**\n",
    "\n",
    "**Your PR should now show as \"Merged\"** - this is what the TA will check!\n",
    "\n",
    "Run the cell below to verify your branch was merged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify your PR was merged (run after merging on GitHub)\n",
    "!git checkout main\n",
    "!git pull\n",
    "!git log --oneline -3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## BONUS: Full Pipeline (10 bonus points)\n",
    "\n",
    "Put everything together! Create a complete pipeline that takes a movie description and returns both the structured data AND a generated poster.\n",
    "\n",
    "**Challenge:** Write your own original movie description and generate a poster for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS: Create a complete pipeline (10 bonus pts)\n",
    "\n",
    "async def movie_to_poster(description: str) -> tuple[Movie, str]:\n",
    "    \"\"\"\n",
    "    Complete pipeline: description -> structured data -> poster\n",
    "    \n",
    "    Args:\n",
    "        description: A text description of a movie\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (Movie object, path to saved poster image)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with YOUR OWN original movie idea!\n",
    "\n",
    "my_movie_description = \"\"\"\n",
    "YOUR ORIGINAL MOVIE IDEA HERE - BE CREATIVE!\n",
    "Describe the plot, characters, setting, visual style, and mood.\n",
    "\"\"\"\n",
    "\n",
    "# Uncomment to run:\n",
    "# movie, poster_path = await movie_to_poster(my_movie_description)\n",
    "# print(f\"Generated poster for: {movie.title}\")\n",
    "# print(movie.model_dump_json(indent=2))\n",
    "# display(Image(poster_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Submission Checklist\n",
    "\n",
    "Before submitting, make sure:\n",
    "\n",
    "- [ ] All code cells run without errors\n",
    "- [ ] Your `Movie` schema includes all required fields with proper validation\n",
    "- [ ] `extract_movie()` returns a valid `Movie` object\n",
    "- [ ] Async processing works and shows timing\n",
    "- [ ] Temperature comparison shows deterministic vs random outputs\n",
    "- [ ] Logprobs visualization works and displays token probabilities\n",
    "- [ ] Reasoning model comparison shows differences between o3-mini and gpt-5-mini\n",
    "- [ ] You generated and displayed at least one movie poster\n",
    "- [ ] Created branch `homework-2` and pushed to GitHub\n",
    "- [ ] Opened a Pull Request from `homework-2` to `main`\n",
    "- [ ] **Merged the PR** (it should show as \"Merged\" on GitHub)\n",
    "- [ ] Submitted notebook on Blackboard\n",
    "\n",
    "**Submission:**\n",
    "1. Complete all tasks in this notebook\n",
    "2. Create a PR and **merge it** on GitHub\n",
    "3. Submit your notebook (`.ipynb` file) on **Blackboard**\n",
    "\n",
    "**The TA will verify your submission by checking the merged PR on your GitHub repo.**\n",
    "\n",
    "---\n",
    "\n",
    "**Great work!** You've built a complete AI-powered application, explored LLM parameters and reasoning, and learned a professional Git workflow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
